{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4b MA : Build your own corpus exploration tool\n",
    "\n",
    "**Deadline for Assignment 4a+b: Friday, October 9, 2020 (17.00) via Canvas (Assignment 4)** \n",
    "\n",
    "\n",
    "In this assignment, you're going to build your own tool for exploring a the **Parallel Meaning Bank** (PMB). This resource is a **parallel corpus**, which means that it contains the **same documents translated into multiple languages**. Such resources are very valuable for many aspects of linguistics and Natural Language Processing (NLP), but most importantly for Machine Translation (ML). \n",
    "\n",
    "For this part of assignment 4, you will submit two python scripts called:\n",
    "\n",
    "* `explore_pmb.py`\n",
    "* `utils.py`\n",
    "\n",
    "The corpus contains a lot of data, but not every document is translated into every language. Therefore, we will build a tool which explores different aspects of coverage. Your tool will be able to:\n",
    "\n",
    "* explore the **overall coverage per language**\n",
    "* explore the the **parallel coverage of a given language pair** (i.e. how many documents and tokens exist in a language pair?)\n",
    "* **browse parallel text** in given language pairs\n",
    "\n",
    "Before diving into building the tool, we're going to guide you through a couple of warm-up examples. You can use them to explore the data structure and write your code. It is permitted to copy-paste bits of code (you will have to modify them to solve all exercises). \n",
    "\n",
    "The assignment is structured as follows:\n",
    "\n",
    "1. Understanding the data structure (code snippets to guide you through the corpus)\n",
    "2. Writing functions (writing the actual code)\n",
    "3. Putting the tool together (combining the code)\n",
    "4. Testing and submission (a final check of whether your code does what it is supposed to do)\n",
    "\n",
    "\n",
    "You can learn more about the PMB [here](https://pmb.let.rug.nl/). \n",
    "\n",
    "If you have **questions** about this chapter, please contact us at cltl.python.course@gmail.com. Questions and answers will be collected in [this Q&A document](https://docs.google.com/document/d/1551Db87zckRPbKDosZ4105htEUxVWZu9ejDj3MM8qck/edit?usp=sharing), so please check it before you email. \n",
    "\n",
    "**Tip**: Read the entire assignment before you start writing code. Try to understand the tool we're building before you start. Making notes with pen and paper can be very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the data structure\n",
    "\n",
    "In this part, we guide you through the data structure. You can use the code below for the rest of your assignment. You can play with the code and add things to it, but you will not receive points in this part. Its purpose is to make you familiar with the data structure.  \n",
    "\n",
    "### 1.a Download the data\n",
    "\n",
    "1.) Please go to this website: [here](https://pmb.let.rug.nl/data.php)\n",
    "\n",
    "2.) Select version 2.1.0 (the latest version is too big for our purposes) and store the zip file as `PMB/pmb-2.1.0.zip` on your computer (remember where).\n",
    "\n",
    "3.) Unpack the data. You can do this from the terminal by navigating to the directory using `cd`. You should be able to run `unzip pmb-2.1.0.zip` to unzip the file. Alternatively, you can simply unzip by clicking on it. Attention: Unpacking the file may take a while. \n",
    "\n",
    "Use the cell below to assign the path to the data to a variable. We will only consider the gold data for this assignment, therefore you can add the gold directory to the path.\n",
    "\n",
    "Path: `'PMB/pmb-2.1.0/data/gold/'`\n",
    "\n",
    "**Please run the following cell to check if your data are in the right place. If they are, it will not print anything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#my_path = #insert path to the directory containing the PMB on your computer\n",
    "# e.g.:\n",
    "# my_path = '/Users/piasommerauer/Data/'\n",
    "\n",
    "path_pmb = 'PMB/pmb-2.1.0/data/gold'\n",
    "assert os.path.isdir(path_pmb), 'corpus data not found'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Parallel documents \n",
    "Before we can build anything, we have to understand how the data are strucutred. We start by looking at a single document. \n",
    "\n",
    "Parallel documents are stored in the same document directory (d+number). The filenames indicate the language (e.g. en = English). The data we're interested in are stored in .xml format. Run the cell below to inspect the filepaths of a single document. Feel free to modify the path to inspect other documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMB/pmb-2.1.0/data/gold/p27/d0852/en.drs.xml\n",
      "PMB/pmb-2.1.0/data/gold/p27/d0852/de.drs.xml\n",
      "PMB/pmb-2.1.0/data/gold/p27/d0852/nl.drs.xml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/'\n",
    "test_doc_files = glob.glob(f'{test_doc_path}*.xml')\n",
    "\n",
    "for f in test_doc_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c XML structure of a single document\n",
    "\n",
    "Below, we access a single document and load the xml structure using lxml.etree. Run the cell to print the xml tree. \n",
    "\n",
    "Explore the document structure and try to answer these questions:\n",
    "\n",
    "* Where can you find the full text of the document?\n",
    "* Where can you find information about each token in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMB/pmb-2.1.0/data/gold/p27/d0852/\n",
      "<lxml.etree._ElementTree object at 0x7fa792a84600>\n",
      "<Element xdrs-output at 0x7fa792a849c0>\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "test_doc_path_en = test_doc_path+'de.drs.xml'\n",
    "print(test_doc_path)\n",
    "doc_tree = etree.parse(test_doc_path_en)\n",
    "print(doc_tree)\n",
    "doc_root = doc_tree.getroot()\n",
    "print(doc_root)\n",
    "#etree.dump(doc_root, pretty_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing functions\n",
    "\n",
    "In this part of the assigment, we guide you through writing the functions for your tool. Feel free to use the notebook for exploration, but your final functions should be stored in `utils.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Get all token elements of a document in a given language\n",
    "\n",
    "Write a function which fulfills the following requirements: \n",
    "\n",
    "* Positional parameter: path to the document in a particular lanugage \n",
    "* Output: list of token elements (the token elements are called 'tagtoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element tagtoken at 0x7fa792c55280>, <Element tagtoken at 0x7fa792c552c0>, <Element tagtoken at 0x7fa792c55300>, <Element tagtoken at 0x7fa792c55340>, <Element tagtoken at 0x7fa792c55380>, <Element tagtoken at 0x7fa792c553c0>]\n"
     ]
    }
   ],
   "source": [
    "def get_tokens(path_to_doc):\n",
    "    '''\n",
    "    todo docstring\n",
    "    '''\n",
    "    from lxml import etree\n",
    "    \n",
    "    doc_tree = etree.parse(path_to_doc)\n",
    "    \n",
    "    doc_root = doc_tree.getroot()\n",
    "    return doc_root.findall('.//tagtoken')\n",
    "    #etree.dump(doc_root, pretty_print=True)\n",
    "    \n",
    "    # your code here \n",
    "    pass \n",
    "    \n",
    "# Test you function\n",
    "test_part = 'p27'\n",
    "test_document = 'd0852'\n",
    "language = 'en'\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/{language}.drs.xml'\n",
    "# Function call\n",
    "tokens = get_tokens(test_doc_path)\n",
    "print(tokens)\n",
    "assert len(tokens) == 6 and type(tokens[1]) == etree._Element, 'token list not correct'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Get token and pos from a token element\n",
    "\n",
    "Write a function which fulfills the following requirements: \n",
    "\n",
    "* Positional parameter: token element\n",
    "* Output: token (string) and part of speech tag (string) of the token element\n",
    "\n",
    "An example token element is shown below. (You can use it for testing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tagtoken at 0x7fa792c4f040>\n"
     ]
    }
   ],
   "source": [
    "test_token_str = \"\"\"\n",
    " <tagtoken xml:id=\"i1002\">\n",
    "   <tags>\n",
    "     <tag type=\"verbnet\" n=\"0\">[]</tag>\n",
    "     <tag type=\"tok\">'m</tag>\n",
    "     <tag type=\"sym\">be</tag>\n",
    "     <tag type=\"lemma\">be</tag>\n",
    "     <tag type=\"from\">1</tag>\n",
    "     <tag type=\"to\">3</tag>\n",
    "     <tag type=\"pos\">VBP</tag>\n",
    "     <tag type=\"sem\">NOW</tag>\n",
    "     <tag type=\"wordnet\">O</tag>\n",
    "   </tags>\n",
    " </tagtoken>\n",
    "\"\"\"\n",
    "\n",
    "test_token = etree.fromstring(test_token_str)\n",
    "print(test_token)\n",
    "\n",
    "\n",
    "# lijstje = []\n",
    "# for i in test_token.iter():\n",
    "#     lijstje.append(i)\n",
    "# print(lijstje[].text)\n",
    "\n",
    "#doc_root = test_token.findall('pos')\n",
    "#doc_root\n",
    "#return doc_root.findall('.//tagtoken')\n",
    "#etree.dump(doc_root, pretty_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_pos(token_element):\n",
    "    '''\n",
    "    todo docstring\n",
    "    '''\n",
    "    lijstje = []\n",
    "    for i in test_token.iter():\n",
    "        lijstje.append(i)\n",
    "    return lijstje[3].text, lijstje[8].text\n",
    "\n",
    "pass\n",
    "\n",
    "# Test your function using the first token \n",
    "token, pos = get_token_pos(test_token)\n",
    "assert token == \"'m\" and pos == 'VBP', 'token and pos not correct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Get document text\n",
    "\n",
    "Define a function with the following requirements:\n",
    "\n",
    "* Positional parameter: filepath of a document in a particular language (i.e. full, relativ filepath)\n",
    "* Output: the text of the document as a string\n",
    "\n",
    "**Hint**:\n",
    " \n",
    "There are two options to get the document text of a file:\n",
    "\n",
    "* Option 1: Access the comment indicated by `<!--  -->`. Look at the file above to find the comment. You will see that it contains the entire text represented in the xml file. You can access it by iterating over the child-elements of the root. Try this out in the notebook before defining your function. You can get started using the code below.\n",
    "\n",
    "\n",
    "* Option 2: Use the tokens. You can collect all the tokens in a document using the functions we have defined above. Once you have all tokens, you can use a string method to join them with whitespaces between them.\n",
    "\n",
    "Only implement **one** of these options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Hij brouwde ø bier . ', '\\n ']\n"
     ]
    }
   ],
   "source": [
    "# Code snippet for option 1 \n",
    "# use the test document\n",
    "test_doc_path_en = 'PMB/pmb-2.1.0/data/gold/p03/d0884/nl.drs.xml'\n",
    "# load\n",
    "doc_tree = etree.parse(test_doc_path_en)\n",
    "# get root \n",
    "root = doc_tree.getroot()\n",
    "# iterate over child-elements\n",
    "list_of_tokens = []\n",
    "for ch in root.getchildren():\n",
    "    #print('tag', ch.tag)\n",
    "    list_of_tokens.append(ch.text)\n",
    "print(list_of_tokens)\n",
    "    \n",
    "    \n",
    "for name in root.iter(\"./tok\"):\n",
    "    etree.dump(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hij brouwde ø bier .'\n"
     ]
    }
   ],
   "source": [
    "def get_doc_text(path_to_doc):\n",
    "    '''\n",
    "    todo doc\n",
    "    '''\n",
    "    doc_tree = etree.parse(path_to_doc)\n",
    "    root = doc_tree.getroot()\n",
    "    list_of_tokens = []\n",
    "    for ch in root.getchildren():\n",
    "    \n",
    "        list_of_tokens.append(ch.text)\n",
    "    #print(list_of_tokens)\n",
    "    return \"\".join(list_of_tokens).strip()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     tokens = get_tokens(path_to_doc)\n",
    "#     strings = []\n",
    "#     for token in tokens:\n",
    "#         #print(tokens)\n",
    "#         for tags in token.findall('tags'):\n",
    "#             tags_per_token = []\n",
    "#             for tag in tags:\n",
    "#                 tags_per_token.append(tag.text)\n",
    "#             strings.append(tags_per_token[1])\n",
    "#     return \" \".join(strings)\n",
    "\n",
    "## testing\n",
    "test_part = 'p03'\n",
    "test_document = 'd0884'\n",
    "language = 'nl'\n",
    "test_doc_path = f'{path_pmb}/{test_part}/{test_document}/{language}.drs.xml'\n",
    "\n",
    "text = get_doc_text(test_doc_path)\n",
    "print(repr(text))\n",
    "\n",
    "#assert text == \"I 'm not tired at~all .\", 'doc text not correct'\n",
    "\n",
    "\n",
    "#get_doc_text('PMB/pmb-2.1.0/data/gold/p03/d0884/nl.drs.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I \\'m not tired at~all .\"'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(\"I 'm not tired at~all .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d Sort documents on languages \n",
    "\n",
    "To explore the coverage of the corpus, it is convenient to store the documents as sets mapped to their language. We can then use set methods (i.e. intersection) to check which documents exist in a given language pair. \n",
    "\n",
    "Write a function which fulfills the following criteria:\n",
    "\n",
    "* mandatory positional argument: path to the corpus (e.g. '../../../Data/PMB/pmb-2.1.0/data/gold')\n",
    "* output: a dictionary of the following format:\n",
    "            `{\n",
    "              'language1': {'path_to_doc1', 'path_to_doc2', ...},\n",
    "              'language2': {'path_to_doc1', 'path_to_doc4', ...},\n",
    "              'language3': {'path_to_doc2', 'path_to_doc3', ...},\n",
    "              }`\n",
    "       \n",
    "       \n",
    "Hints:\n",
    "\n",
    "* filepaths are strings; you can manipulate them using string methods (e.g. split on '/' or '.'). \n",
    "* The os mudule has a convenient way of extracting the filename from a long path (i.e. the last bit of the path): os.path.basename(your_path)\n",
    "* Feel free to use [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) (with a set as the default value) (`from collections import defaultdict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name.txt\n",
      "../../some/dir/containing/a/file/with/an/interesting/\n",
      "name txt\n"
     ]
    }
   ],
   "source": [
    "# Example for filepath manipulation:\n",
    "import os\n",
    "\n",
    "\n",
    "my_path = '../../some/dir/containing/a/file/with/an/interesting/name.txt'\n",
    "f_name = os.path.basename(my_path)\n",
    "print(f_name)\n",
    "remaining_path = my_path.rstrip(f_name)\n",
    "print(remaining_path)\n",
    "name, extension = f_name.split('.')\n",
    "print(name, extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4555, 635, 1175, 586)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "en = set()\n",
    "nl = set()\n",
    "de = set()\n",
    "it = set()\n",
    "#break_counter = 0\n",
    "for d_folder in glob.glob(path_pmb+'/*/*'):\n",
    "    for language_xml in glob.glob(d_folder+'/*.xml'):\n",
    "        #print(d_folder,os.path.basename(language_xml)[:2] )\n",
    "        if os.path.basename(language_xml)[:2] == 'en':\n",
    "            en.add(language_xml)\n",
    "        elif os.path.basename(language_xml)[:2] == 'nl':\n",
    "            nl.add(language_xml)\n",
    "        elif os.path.basename(language_xml)[:2] == 'de':\n",
    "            de.add(language_xml)\n",
    "        else:\n",
    "            it.add(language_xml)\n",
    "    #break_counter +=1\n",
    "    #if break_counter > 50:\n",
    "       # break\n",
    "    #print()\n",
    "    \n",
    "len(en), len(it), len(de), len(nl)\n",
    "    \n",
    "    \n",
    "    #print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['en', 'nl', 'de', 'it'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path_pmb = 'PMB/pmb-2.1.0/data/gold'\n",
    "\n",
    "def sort_documents(path_pmb):\n",
    "    result = {}\n",
    "    import glob\n",
    "    en = set()\n",
    "    nl = set()\n",
    "    de = set()\n",
    "    it = set()\n",
    "    #break_counter = 0\n",
    "    for d_folder in glob.glob(path_pmb+'/*/*'):\n",
    "        for language_xml in glob.glob(d_folder+'/*.xml'):\n",
    "            #print(d_folder,os.path.basename(language_xml)[:2] )\n",
    "            if os.path.basename(language_xml)[:2] == 'en':\n",
    "                en.add(language_xml)\n",
    "            elif os.path.basename(language_xml)[:2] == 'nl':\n",
    "                nl.add(language_xml)\n",
    "            elif os.path.basename(language_xml)[:2] == 'de':\n",
    "                de.add(language_xml)\n",
    "            else:\n",
    "                it.add(language_xml)\n",
    "    result['en'] = en\n",
    "    result['nl'] = nl\n",
    "    result['de'] = de\n",
    "    result['it'] = it\n",
    "    return result\n",
    "        #break_counter +=1\n",
    "        #if break_counter > 50:\n",
    "           # break\n",
    "        #print()\n",
    "\n",
    "\n",
    "# Test you function:\n",
    "language_doc_dict = sort_documents(path_pmb)\n",
    "\n",
    "n_en = len(language_doc_dict['en'])\n",
    "n_it = len(language_doc_dict['it'])\n",
    "n_de = len(language_doc_dict['de'])\n",
    "n_nl = len(language_doc_dict['nl'])\n",
    "\n",
    "assert n_en == 4555 and n_it == 635 and n_de == 1175 and n_nl == 586, 'sorting not correct'\n",
    "language_doc_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Putting the tool together\n",
    "\n",
    "Congratulations! You've written most of the code already! \n",
    "\n",
    "From now on, we will mostly use the functions defined above and combine them in the file called `explore_pmb.py`. \n",
    "\n",
    "Some code snippets are provided here to help you along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Printing statistics for all languages\n",
    "\n",
    "Let's start by exploring the coverage of all languages individually. In `explore_pmb.py`, write the following code:\n",
    "\n",
    "* Import the function`sort_docs`, call it and assign the output dictionary to a variable called `language_doc_dict`. Don't forget to define the path to the corpus, which you use as function input. \n",
    "* Create a list of all lanugages (hint: you can simply use the keys of `language_doc_dict`). \n",
    "* For each lanugage, print the following:\n",
    "    `[Language]: num docs: [number of documents], num tokens: [number of tokens]\n",
    "    \n",
    "Hints:\n",
    "\n",
    "* Each document is unique - you can simply count the elements in the sets to get the number of documents.\n",
    "* Use the function `get_tokens` to access the tokens. Then count them.\n",
    "* You will most likly use two nested loops: An outer loop for languages and an inner loop to access the tokens in the documents. \n",
    "* Use f-strings to print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: num docs: 4555, num tokens: 30452\n",
      "nl: num docs: 586, num tokens: 3702\n",
      "de: num docs: 1175, num tokens: 7158\n",
      "it: num docs: 635, num tokens: 3741\n"
     ]
    }
   ],
   "source": [
    "path_pmb = 'PMB/pmb-2.1.0/data/gold'\n",
    "language_doc_dict = sort_documents(path_pmb)\n",
    "languages = language_doc_dict.keys()\n",
    "\n",
    "for language in languages:\n",
    "    n_docs = len(language_doc_dict[language])\n",
    "    n_tokens = 0\n",
    "    for path in language_doc_dict[language]:\n",
    "        n_tokens += len(get_tokens(path))\n",
    "#     docs = language_doc_dict[language]\n",
    "#     print(language, n_docs, n_tokens)\n",
    "#     print('/n/n/n')\n",
    "#     your code here\n",
    "#     for doc in docs:\n",
    "#         path_to_doc = f'{doc}/{language}.drs.xml'\n",
    "#         tokens = get_tokens(path_to_doc)\n",
    "#         # your code here\n",
    "    print(f'{language}: num docs: {n_docs}, num tokens: {n_tokens}')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Printing statistics for language pairs \n",
    "\n",
    "We also want to explore the coverage of **parallel data** for the lanugage pairs in the corpus. To do this, use an additional loop to iterate over all possible lanugage pairs in the parallal meaning bank and print the number of documents which exist for both languages. \n",
    "\n",
    "Use the function below to generate the lanugage pairs. Simply copy-paste it into the script called `utils.py` and import it into `explore_pmb.py`. Use the cell below to explore how it works. \n",
    "\n",
    "The list of language pair should look similar to this (and contain all possible pairs):\n",
    "\n",
    "`pairs = [(‘nl’, ‘en’), (‘it’, ‘de’), (‘en’, ‘it’)]`\n",
    "\n",
    "Print the following for each lanugage pair (use f-strings):\n",
    "\n",
    "`Coverage for parallel data in [language_1] and [language_2]: [number of documents]`\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You can unpack tuples in a loop.\n",
    "* Use a set method to get the document paths covered by both languages. Then simply count the paths.\n",
    "* You do not have to match the file-contents. Instead, use the information provided in the filepaths (in a previous step, you have sorted your corpus files according to language).  The file paths in the sets (representing the documents) are supposed to consist of the base names only (i.e. no directory paths). You can use set operations to get the overlap between two languages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 'nl'), ('en', 'de'), ('en', 'it'), ('nl', 'de'), ('nl', 'it'), ('de', 'it')]\n"
     ]
    }
   ],
   "source": [
    "def get_pairs(language_list):\n",
    "    \"\"\"Given a list, return a list of tuples of all element pairs.\"\"\"\n",
    "    pairs = []\n",
    "    for l1 in language_list:\n",
    "        for l2 in language_list:\n",
    "            if l1 != l2:\n",
    "                if (l1, l2) not in pairs and (l2, l1) not in pairs:\n",
    "                    pairs.append((l1, l2))\n",
    "    return pairs\n",
    "\n",
    "language_list = ['en', 'nl', 'de', 'it']\n",
    "pairs = get_pairs(language_list)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_doc_dict = sort_documents(path_pmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage for parallel data in en and nl: 581\n",
      "Coverage for parallel data in en and de: 1171\n",
      "Coverage for parallel data in en and it: 633\n",
      "Coverage for parallel data in nl and de: 265\n",
      "Coverage for parallel data in nl and it: 139\n",
      "Coverage for parallel data in de and it: 241\n"
     ]
    }
   ],
   "source": [
    "# Here's a start (feel free to modify this)\n",
    "\n",
    "for lang1, lang2 in pairs:\n",
    "    docs_lang1 = language_doc_dict[lang1]\n",
    "    docs_lang2 = language_doc_dict[lang2]\n",
    "    #print(docs_lang1,'/n/n',docs_lang2)\n",
    "    stripped_lang1 = [path[:-11] for path in docs_lang1]\n",
    "    stripped_lang2 = [path[:-11] for path in docs_lang2]\n",
    "    set_lang1 = set(stripped_lang1)\n",
    "    set_lang2 = set(stripped_lang2)\n",
    "    intersection = set_lang1.intersection(set_lang2)\n",
    "    print(f'Coverage for parallel data in {lang1} and {lang2}: {len(intersection)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Explore parallel documents \n",
    "\n",
    "As a final step, we want let the user browse the parallel documents in a chosen language pair. Write the following code (in `explore_pmb.py`):\n",
    "\n",
    "* use input() to define two variables: language_1 and language_2\n",
    "* get the document paths for all documents covered by both languages\n",
    "* Loop over the documents and print the documents in both lanugages. After each document, ask the user whether they want to continue. If the answer is 'no', stop. Else, show the next. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Come up with your own comparison\n",
    "\n",
    "Got insterested in parallel data? Extract a comparison you find interesting! \n",
    "\n",
    "**This is an additional exercise - it is not required to complete this to get a full score.** \n",
    "\n",
    "If you complete this exercise, you can earn up to 3 additional points which can be used to make up for other points you missed. Note that you cannot get more than a full score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing & submission\n",
    "\n",
    "Congratulations! You've built a corpus exploration tool! Before you submit, please make sure to test your code:\n",
    "\n",
    "* Can you run the script `explore_pmb.py` from the command line?\n",
    "* Do you get a general corpus overview (see 3.a)?\n",
    "* Do you get an overview of language pairs (see 3.b)?\n",
    "* Are you asked to provide a lanugage pair and do you see examples of parallel documents once you entered a pair (see 3.c?)\n",
    "\n",
    "If you did not manage to complete all of the exercises, submit what you have and, if possible, explain how you were going to solve them. You get points for intermediate steps. \n",
    "\n",
    "**Please submit python scripts only. You can use this notebook for exploration and development, but we will not consider the code written here.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please provide the first languagenl\n",
      "please provide the second languageen\n",
      "PMB/pmb-2.1.0/data/gold/p03/d0884/nl.drs.xml\n",
      "Text in nl: Hij brouwde ø bier .\n",
      "Text in en: He brewed ø beer .\n",
      " do you want to continue? yes/no\n",
      "PMB/pmb-2.1.0/data/gold/p79/d0914/nl.drs.xml\n",
      "Text in nl: Je hebt alles verpest .\n",
      "Text in en: You ruined everything .\n",
      " do you want to continue? yes/no\n",
      "PMB/pmb-2.1.0/data/gold/p95/d0953/nl.drs.xml\n",
      "Text in nl: ø Tom morste de melk .\n",
      "Text in en: ø Tom spilled the milk .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8e7f84c9289e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mparallel_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-8e7f84c9289e>\u001b[0m in \u001b[0;36mparallel_documents\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Text in {lang2}: {text_lang2}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mcontinue_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' do you want to continue? yes/no'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontinue_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def parallel_documents():\n",
    "    language_doc_dict = sort_documents(path_pmb)\n",
    "    lang1 = input('please provide the first language:  ')\n",
    "    lang2 = input('please provide the second language:  ')\n",
    "    \n",
    "    docs_lang1 = language_doc_dict[lang1]\n",
    "    docs_lang2 = language_doc_dict[lang2]\n",
    "    #print(docs_lang1,'/n/n',docs_lang2)\n",
    "    stripped_lang1 = [path[:-11] for path in docs_lang1]\n",
    "    stripped_lang2 = [path[:-11] for path in docs_lang2]\n",
    "    set_lang1 = set(stripped_lang1)\n",
    "    set_lang2 = set(stripped_lang2)\n",
    "    intersection = set_lang1.intersection(set_lang2)\n",
    "    \n",
    "    intersect_lang1 = [path_without_xml + \"/\"+lang1+'.drs.xml' for path_without_xml in intersection]\n",
    "    intersect_lang2 = [path_without_xml + \"/\"+lang2+'.drs.xml' for path_without_xml in intersection]\n",
    "    \n",
    "    for path_lang1, path_lang2 in tuple(zip(intersect_lang1, intersect_lang2)):\n",
    "        print(path_lang1)\n",
    "        text_lang1 = get_doc_text(path_lang1)\n",
    "        text_lang2 = get_doc_text(path_lang2)\n",
    "        print(f'Text in {lang1}: {text_lang1}')\n",
    "        print(f'Text in {lang2}: {text_lang2}')\n",
    "        \n",
    "        continue_input = input(' do you want to continue? yes/no')\n",
    "        if continue_input == 'no':\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "parallel_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
